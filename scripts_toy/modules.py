import torch
import torch.nn.functional as F 
import numpy as np 
import os


def generate_base_grid(phi):
    """
    Define a base grid
    Args:
        phi: displacement field in shape [batch, x, y, z, channel]
    Return a base grid with no gradient recorded
    """
    base_grid = torch.meshgrid(torch.linspace(-1,1,phi.shape[1]),
                               torch.linspace(-1,1,phi.shape[2]),
                               torch.linspace(-1,1,phi.shape[3]))
    # torch seems to want [z, y, x] displacements
    # applying base_grid only without this rearrangement results in axis reordering
    base_grid = torch.stack((base_grid[2], base_grid[1], base_grid[0]))
    base_grid = base_grid.permute(1,2,3,0)  # [x, y, z, channel]
    base_grid = base_grid.unsqueeze(0)  # [batch, x, y, z, channel]
    base_grid = base_grid.to(phi.dtype)
    base_grid = base_grid.to(phi.device)
    return base_grid


def transform_layer(img, phi):
    """
    Transformation layer. Phi is a displacement field.
    Args:
        img: images in shape [batch, channel, x, y, z], only first channel is the raw image
        phi: displacement field in shape [batch, channel, x, y, z]
    """
    phi = phi.permute(0,2,3,4,1)  # [batch, x, y, z, channel]
    base_grid = generate_base_grid(phi)
    phi += base_grid
    img = torch.split(img, 1, dim=1)  # split channel, the first channel is raw img
    warped = F.grid_sample(img[0], phi)
    return warped


def cc_loss(output, target, phi=None, lamda=1e-5):
    """
    Pearson correlation loss with smooth control of phi
    """
    def calculate_gradient(phi):
        """
        phi in shape [batch, channel, x, y, z]
        """
        if phi is not None:
            grad_xc = 0.5 * (phi[:,:,2:,:,:] - phi[:,:,:-2,:,:])
            grad_xn = (phi[:,:,-1,:,:] - phi[:,:,-2,:,:]).unsqueeze(2)
            grad_x0 = (phi[:,:,1,:,:] - phi[:,:,0,:,:]).unsqueeze(2)
            grad_x = torch.cat((grad_x0,grad_xc,grad_xn), dim=2)

            grad_yc = 0.5 * (phi[:,:,:,2:,:] - phi[:,:,:,:-2,:])
            grad_yn = (phi[:,:,:,-1,:] - phi[:,:,:,-2,:]).unsqueeze(3)
            grad_y0 = (phi[:,:,:,1,:] - phi[:,:,:,0,:]).unsqueeze(3)
            grad_y = torch.cat((grad_y0,grad_yc,grad_yn), dim=3)

            grad_zc = 0.5 * (phi[:,:,:,:,2:] - phi[:,:,:,:,:-2])
            grad_zn = (phi[:,:,:,:,-1] - phi[:,:,:,:,-2]).unsqueeze(4)
            grad_z0 = (phi[:,:,:,:,1] - phi[:,:,:,:,0]).unsqueeze(4)
            grad_z = torch.cat((grad_z0,grad_zc,grad_zn), dim=4)

            grad_xyz   = torch.cat([grad_x, grad_y, grad_z], dim=1)
            grad_phi   = torch.sum(torch.sqrt(torch.sum(grad_xyz**2, dim=1)).unsqueeze(1), dim=(2,3,4))
            grad_phi = torch.mean(grad_phi)
            return grad_xyz, grad_phi
        else:
            pass
    
    x = output - torch.mean(output, dim=(2,3,4), keepdim=True)
    y = target - torch.mean(target, dim=(2,3,4), keepdim=True)
    cc = torch.sum(x * y, dim=(1,2,3,4)) / (torch.sqrt(torch.sum(x ** 2, dim=(1,2,3,4))) *
                                            torch.sqrt(torch.sum(y ** 2, dim=(1,2,3,4))))
    cc = torch.mean(cc)
    grad = 0
    if phi is not None:
        grad, smooth = calculate_gradient(phi)
        lamda = torch.tensor([lamda])
        lamda = lamda.to(phi.dtype)
        lamda = lamda.to(phi.device)
        loss = -cc + lamda * smooth
    else:
        loss = -cc
    return loss, grad

class ImgRegisterNetwork():
    """
    Image registeration network class that wraps model related functions (e.g., training, evaluation, etc)
    """
    def __init__(self, model, criterion, optimizer, device):
        """
        Args:
            model: a deep neural network model (sent to device already)
            criterion: loss function
            optimizer: training optimizer
            device: training device
        """
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device


    def train_model(self, data, i):
        """
        Train the model
        Args:
            data: training dataset generated by DataLoader
        Return batch-wise training loss
        """

        import nrrd

        self.model.train()
        training_loss = 0

        for batch, [img, tmplt] in enumerate(data):
            img = img.to(self.device)
            tmplt = tmplt.to(self.device)

            # Forward
            phi = self.model(img)

            if i % 50 == 0 or i == 199:
                transform = phi.clone().cpu().detach().numpy().squeeze()[[2, 1, 0], ...]
                transform = np.moveaxis(transform, 0, -1) * 63./2.
                nrrd.write('./transform_batch'+str(batch)+'_it'+str(i)+'.nrrd', transform)

            # Apply transformation layer
            warped = transform_layer(img, phi)

            if i % 50 == 0 or i == 199:
                nrrd.write('./warped_batch'+str(batch)+'_it'+str(i)+'.nrrd', warped.cpu().detach().numpy().squeeze())

            # Calculate loss
            loss, _ = self.criterion(warped, tmplt, phi)
            training_loss += loss.item()
            # Zero the parameter gradients
            self.optimizer.zero_grad()
            # Backward
            loss.backward()
            # Update weights
            self.optimizer.step()

        print(str(i) + ":  Batch-wise training loss for current epoch is {}".format(training_loss/(batch+1)))
        return training_loss/(batch+1)

class ImgRegisterNetworkDfield():
    """
    Image registration network class that wraps model related functions (e.g., training, evaluation, etc)
    """
    def __init__(self, model, criterion, optimizer, device):
        """
        Args:
            model: a deep neural network model (sent to device already)
            criterion: loss function
            optimizer: training optimizer
            device: training device
        """
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device

    def train_model(self, data, i):
        """
        Train the model
        Args:
            data: training dataset generated by DataLoader
        Return batch-wise training loss
        """

        self.model.train()
        training_loss = 0

        for batch, [img, dfield] in enumerate(data):
            img = img.to(self.device)
            dfield = dfield.to(self.device)

            # Forward
            phi = self.model(img)

            #if i % 50 == 0 or i == 199:
            #    transform = phi.clone().cpu().detach().numpy().squeeze()[[2, 1, 0], ...]
            #    transform = np.moveaxis(transform, 0, -1) * 63./2.
            #    nrrd.write('./transform_batch'+str(batch)+'_it'+str(i)+'.nrrd', transform)

            #if i % 50 == 0 or i == 199:
            #    nrrd.write('./warped_batch'+str(batch)+'_it'+str(i)+'.nrrd', warped.cpu().detach().numpy().squeeze())

            # Calculate loss
            loss, _ = self.criterion(dfield, phi)
            training_loss += loss.item()
            # Zero the parameter gradients
            self.optimizer.zero_grad()                    
            # Backward
            loss.backward()
            # Update weights
            self.optimizer.step()

        print(str(i) + ":  Batch-wise training loss for current epoch is {}".format(training_loss/(batch+1)))
        return training_loss/(batch+1)


# ONLY TESTED UP TO HERE


    def eval_model(self, data):
        """
        Evaluate the model
        Args:
            data: evaluation dataset generated by DataLoader
        Return batch-wise evaluation loss
        """
        self.model.eval()
        eval_loss = 0

        for batch, [img, tmplt] in enumerate(data):
            with torch.no_grad():  # Disable gradient computation
                img = img.to(self.device)
                tmplt = tmplt.to(self.device)
                phi = self.model(img)
                warped = transform_layer(img, phi)
                loss = self.criterion(warped, tmplt, phi)
                eval_loss += loss.item()

        print("Batch-wise evaluation loss for current epoch is {}".format(eval_loss/(batch+1)))
        return eval_loss/(batch+1)


    def save_model(self, path, epoch, entire=False):
        """
        Save the model to disk
        Args:
            path: directory to save the model
            epoch: epoch that model is saved
            entire: if save the entire model rather than just save the state_dict
        """
        if not os.path.exists(path):
            os.mkdir(path)
        if entire:
            torch.save(self.model, path+"/whole_model_epoch_{}.pt".format(epoch))
        else:
            # torch.save(self.model.state_dict(), path+"/model_ckpt_{}.pt".format(epoch))
            torch.save({'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'criterion': self.criterion},
                        path+"/model_ckpt_{}.pt".format(epoch))
    

    def test_model(self, checkpoint, img, tmplt, input_sz):
        """
        Test the model on new data
        Args:
            checkpoint: saved checkpoint
            img: testing data (moving image)
            tmplt: template (fixed image)
            input_sz: network input size in (x,y,z) (network input is [batch, channel, x, y, z])
        """
        assert img.shape == tmplt.shape, "moving image doesn't match fixed template shape!"

        ckpt = torch.load(checkpoint)
        # self.model.load_state_dict(ckpt)
        self.model.load_state_dict(ckpt['model_state_dict'])

        self.model.eval()

        phi = np.zeros((3, img.shape[0], img.shape[1], img.shape[2]), dtype=img.dtype)
        warped  = np.zeros(img.shape, dtype=img.dtype)
        for row in range(0, img.shape[0], input_sz[0]):
            for col in range(0, img.shape[1], input_sz[1]):
                for vol in range(0, img.shape[2], input_sz[2]):
                    # Generate 
                    patch_img = np.zeros((1, 2, input_sz[0], input_sz[1], input_sz[2]), dtype=img.dtype)
                    patch_img[0,0,:,:,:] = img[row:row+input_sz[0], col:col+input_sz[1], vol:vol+input_sz[2]]
                    patch_img[0,1,:,:,:] = tmplt[row:row+input_sz[0], col:col+input_sz[1], vol:vol+input_sz[2]]
                    patch_img = torch.from_numpy(patch_img).float()
                    patch_img = patch_img.to(self.device)
                    # Apply model
                    patch_phi = self.model(patch_img)
                    patch_warped = transform_layer(patch_img, patch_phi)

                    patch_phi = patch_phi.cpu()
                    patch_phi = patch_phi.detach().numpy()
                    phi[:, row:row+input_sz[0], col:col+input_sz[1], vol:vol+input_sz[2]] = patch_phi[0,:,:,:,:]
                    patch_warped = patch_warped.cpu()
                    patch_warped = patch_warped.detach().numpy()
                    warped[row:row+input_sz[0], col:col+input_sz[1], vol:vol+input_sz[2]] = patch_warped
        phi = np.transpose(phi, (1,2,3,0))  # [x, y, z, channel]
        return phi, warped

def torch_position_to_dfield( grid, spacing=[1.,1.,1.]):
    """
    Input grid is what is usable by torch.nn.functional.grid_sample
    i.e. in pixel coordinates such that

    input grid is shape [1 nz ny nx 3]
    grid[:,:,:,:,0] holds x grid positions
    grid[:,:,:,:,1] holds y grid positions
    grid[:,:,:,:,2] holds z grid positions

    spacing is size [3] holding [z y x] spacings

    :param grid:
    :param spacing:
    :return:
    """

    # grid is [ 1, z, y, x, 3] (dx, dy, dz)
    sz = torch.tensor(grid.squeeze().size())[:3]
    id_grid = gen_identity_grid(sz.numpy())

    displacement_raw = (grid - id_grid).squeeze()

    # make displacement  [z, y, x, 3] (dz, dy, dz)
    displacement = torch.flip( displacement_raw, [3] )

    sz_mul = 2.0 / (sz.float() - 1.0)
    spacing_t = torch.as_tensor(spacing.copy())  # spacing may be a numpy or python array, or torch
    mul = (spacing_t / sz_mul).reshape(1, 1, 1, 3)

    displacement *= mul
    return displacement

def gen_identity_grid(sz):
    mgrid = torch.meshgrid(
        torch.linspace(-1, 1, sz[2]),
        torch.linspace(-1, 1, sz[1]),
        torch.linspace(-1, 1, sz[0]))

    id_grid_stack = torch.stack(mgrid).permute(3, 2, 1, 0).unsqueeze(0)
    return id_grid_stack

def load_dfield( nrrd_file ):
    """
    Load a displacement field from a nrrd file as a numpy array,
    and permutes axes in a way that python likes.

    Specifically, expect an input of shape (3, Nx, Ny, Nz)
    where dfield[0,x,y,z] gives the displacement in physical units
    along the 'x' axis at position (x,y,z)

    Returns an array of shape (Nz, Ny, Nx, 3),
    where out[z,y,x,0] gives the displacement in physical units
    along the 'z' axis at position (z,y,x)

    :param nrrd_file:
    :return:
    """
    dfield, hdr = nrrd.read(nrrd_file)
    #tmp = np.transpose(dfield, [3, 2, 1, 0])
    #dfield_perm = np.flip(tmp, 3)

    #dfield_perm = np.flip( np.transpose(dfield, [3, 2, 1, 0]), 3)
    #spacing = np.diagonal(hdr['space directions'][1:, :])
    #spacing_perm = np.flipud(spacing).copy()

    dfield_perm = np.transpose(dfield, [1, 2, 3, 0])
    spacing = np.diagonal(hdr['space directions'][1:, :])

    return dfield_perm, spacing
